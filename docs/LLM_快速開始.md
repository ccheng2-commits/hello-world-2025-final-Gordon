# ğŸš€ æœ¬åœ° LLM å¿«é€Ÿéƒ¨ç½²æŒ‡å—ï¼ˆå°å­¸ç”Ÿç´šæ•™ç¨‹ï¼‰

## ğŸ“ æ­¥é©Ÿ 1: ç¢ºèª Ollama å·²ç¶“å®‰è£

æ‚¨å·²ç¶“æœ‰ Ollama äº†ï¼è®“æˆ‘å€‘ç¢ºèªä¸€ä¸‹ï¼š

```bash
# æª¢æŸ¥ Ollama ç‰ˆæœ¬
ollama --version

# æŸ¥çœ‹å·²å®‰è£çš„æ¨¡å‹
ollama list
```

å¦‚æœçœ‹åˆ° `qwen2.5:3b`ï¼Œèªªæ˜æ¨¡å‹å·²ç¶“ä¸‹è¼‰å¥½äº†ï¼âœ…

---

## ğŸ“ æ­¥é©Ÿ 2: ç¬¬ä¸€æ¬¡æ¸¬è©¦ï¼ˆæœ€ç°¡å–®çš„æ–¹æ³•ï¼‰

### æ–¹æ³• A: ç›´æ¥åœ¨å‘½ä»¤è¡Œä½¿ç”¨ï¼ˆæœ€ç°¡å–®ï¼ï¼‰

æ‰“é–‹çµ‚ç«¯ï¼ˆTerminalï¼‰ï¼Œè¼¸å…¥ï¼š

```bash
ollama run qwen2.5:3b
```

ç„¶å¾Œä½ å°±å¯ä»¥ç›´æ¥å’Œ AI å°è©±äº†ï¼ä¾‹å¦‚ï¼š
- è¼¸å…¥ï¼šã€Œä½ å¥½ã€
- è¼¸å…¥ï¼šã€Œè«‹ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€
- è¼¸å…¥ï¼šã€Œé€€å‡ºã€æˆ–æŒ‰ Ctrl+D ä¾†çµæŸå°è©±

### æ–¹æ³• B: ä¸€æ¬¡å•ç­”æ¸¬è©¦

```bash
ollama run qwen2.5:3b "ä½ å¥½ï¼Œè«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±"
```

---

## ğŸ“ æ­¥é©Ÿ 3: ç”¨ Python æ§åˆ¶ LLMï¼ˆé€²éšï¼‰

å¦‚æœä½ æƒ³åœ¨è‡ªå·±çš„ Python ç¨‹åºä¸­ä½¿ç”¨ LLMï¼Œæœ‰å…©ç¨®æ–¹å¼ï¼š

### æ–¹æ³• 1: ä½¿ç”¨å‘½ä»¤è¡Œèª¿ç”¨ï¼ˆä¸éœ€è¦å®‰è£ Python åº«ï¼‰

å‰µå»ºæ–‡ä»¶ `test_llm_simple.py`ï¼š

```python
import subprocess
import json

def ask_ai(question):
    """æœ€ç°¡å–®çš„æ–¹æ³•ï¼šç›´æ¥èª¿ç”¨å‘½ä»¤è¡Œ"""
    result = subprocess.run(
        ['ollama', 'run', 'qwen2.5:3b', question],
        capture_output=True,
        text=True
    )
    return result.stdout

# æ¸¬è©¦
if __name__ == '__main__':
    print("ğŸ¤– é–‹å§‹æ¸¬è©¦...")
    answer = ask_ai("ä½ å¥½ï¼Œè«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±")
    print(f"\nAI å›ç­”ï¼š\n{answer}")
```

é‹è¡Œï¼š
```bash
python3 test_llm_simple.py
```

### æ–¹æ³• 2: ä½¿ç”¨ Ollama APIï¼ˆæ¨è–¦ï¼‰

1. **ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ**ï¼ˆé€šå¸¸å®ƒæœƒè‡ªå‹•å•Ÿå‹•ï¼‰

2. **å®‰è£ Python åº«**ï¼š
```bash
# å¦‚æœåœ¨è™›æ“¬ç’°å¢ƒä¸­
source venv/bin/activate
pip install ollama

# æˆ–è€…ç³»çµ±ç´šå®‰è£ï¼ˆå¦‚æœä¸Šé¢çš„ä¸è¡Œï¼‰
pip3 install --user ollama
```

3. **å‰µå»ºæ¸¬è©¦è…³æœ¬** `test_llm_api.py`ï¼š

```python
import ollama

def chat(prompt):
    """ä½¿ç”¨ Ollama API å’Œ AI å°è©±"""
    response = ollama.chat(model='qwen2.5:3b', messages=[
        {'role': 'user', 'content': prompt}
    ])
    return response['message']['content']

# æ¸¬è©¦
if __name__ == '__main__':
    print("ğŸ¤– æ¸¬è©¦ Ollama API...")
    
    # ç¬¬ä¸€æ¬¡æ¸¬è©¦
    answer = chat("ä½ å¥½ï¼Œè«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±")
    print(f"\nAI: {answer}\n")
    
    # äº¤äº’å¼å°è©±
    print("é–‹å§‹å°è©±ï¼ˆè¼¸å…¥ 'é€€å‡º' çµæŸï¼‰ï¼š")
    while True:
        user_input = input("\nä½ : ")
        if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
            break
        
        answer = chat(user_input)
        print(f"AI: {answer}")
```

é‹è¡Œï¼š
```bash
python3 test_llm_api.py
```

---

## ğŸ“ æ­¥é©Ÿ 4: æ¸¬è©¦ä¸åŒçš„æ¨¡å‹

å¦‚æœä½ æƒ³è©¦è©¦å…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ï¼š

```bash
# æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
ollama list

# ä¸‹è¼‰æ–°æ¨¡å‹ï¼ˆä¾‹å¦‚ Llama 3.2ï¼‰
ollama pull llama3.2:3b

# ä½¿ç”¨æ–°æ¨¡å‹
ollama run llama3.2:3b
```

---

## ğŸ¯ æ¨è–¦çš„æ¨¡å‹ï¼ˆé‡å° 8GB å…§å­˜çš„ MacBook Air M4ï¼‰

| æ¨¡å‹åç¨± | å¤§å° | ç‰¹é» | ä¸‹è¼‰å‘½ä»¤ |
|---------|------|------|---------|
| **qwen2.5:3b** | 1.9GB | ä¸­æ–‡æ”¯æŒæœ€å¥½ â­ | å·²å®‰è£ âœ… |
| llama3.2:3b | 2GB | è‹±æ–‡ç‚ºä¸» | `ollama pull llama3.2:3b` |
| phi3:mini | 2.3GB | é€Ÿåº¦æœ€å¿« | `ollama pull phi3:mini` |

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•åœæ­¢ Ollama æœå‹™ï¼Ÿ

```bash
# æŸ¥çœ‹ Ollama é€²ç¨‹
ps aux | grep ollama

# åœæ­¢ Ollamaï¼ˆå¦‚æœéœ€è¦ï¼‰
pkill ollama
```

### Q2: æ¨¡å‹æ–‡ä»¶åœ¨å“ªè£¡ï¼Ÿ

æ¨¡å‹ä¿å­˜åœ¨ï¼š`~/.ollama/models/`

### Q3: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹ä½¿ç”¨å¤šå°‘å…§å­˜ï¼Ÿ

```bash
# æŸ¥çœ‹ç•¶å‰é‹è¡Œçš„æ¨¡å‹
ollama ps
```

### Q4: å¦‚ä½•æ›´æ–° Ollamaï¼Ÿ

```bash
brew upgrade ollama
```

### Q5: Python ç„¡æ³•å°å…¥ ollama åº«ï¼Ÿ

å¦‚æœ `pip install ollama` å¤±æ•—ï¼Œè©¦è©¦ï¼š
- ä½¿ç”¨è™›æ“¬ç’°å¢ƒ
- ä½¿ç”¨ `pip3 install --user ollama`
- æˆ–ä½¿ç”¨å‘½ä»¤è¡Œæ–¹æ³•ï¼ˆä¸éœ€è¦ Python åº«ï¼‰

---

## ğŸ‰ å®Œæˆï¼

ç¾åœ¨æ‚¨å·²ç¶“å¯ä»¥åœ¨æœ¬åœ°é‹è¡Œ LLM äº†ï¼

**ä¸‹ä¸€æ­¥ï¼š**
- è©¦è©¦ç›´æ¥é‹è¡Œï¼š`ollama run qwen2.5:3b`
- æˆ–è€…åœ¨ Python ç¨‹åºä¸­ä½¿ç”¨ LLM

æœ‰å•é¡Œéš¨æ™‚å•æˆ‘ï¼ğŸ˜Š









